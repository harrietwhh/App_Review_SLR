paper,year,tool/author,tool link,tool description,artifact,tool component,other tool exploited,comparison between these 36 tools,other comparison,evaluation approach,evaluation metric
Classifying User Requirements from Online Feedback in Small Dataset Environments using Deep Learning,2021,Mekala et al.,https://doi.org/10.6084/m9.figshare.14273594,Classifying app reviews as Helpful or Useless in review level and sentence level.,app reviews,Traditional Machine Learning Implementations/ Deep Learning Implementations,no,,"crowdsourcing results of Van Vliet et al. [1], SVM Classier, Naive Bayes Classifier, FastText-based Classifier, ELMo-based Classifier, BERT-based Classifier",existing dataset：using the sample of user reviews and the gold standard of Van Vliet et al. [1] described in Section IV-C.,"Precision, Recall, F1-measure"
AR-miner: mining informative reviews for developers from mobile app marketplace,2014,AR-miner,https://github.com/jinyyy666/AR_Miner (replication package submiited by others),Summarizing app reviews by visualizing the most informative reviews,app reviews,Filtering + Grouping + Ranking + Visualization,no,,"evaluate their own tool:

EMNB-LDA and LDA;

LDA and ASUM;

AR-Miner (EMNB) and AR-Miner (NB)","1. annotated dataset by authors;

2. online forum","Recall (Hit-rate) and F-measure;
Adopt NormalizedDiscounted Cumulative Gain (NDCG) [17] as a measure forevaluating the quality of top-k ranking results;"
Exploiting Natural Language Structures in Software Informal Documentation,2019,NEON,https://github.com/adisorbo/NEONReplicationPackage,Identifying recurrent language structures in texts,"app reviews, development emails, descriptions of bugs mined from GitHub",Pattern Mining + Classification,no,,Rule Identification with NEON and without NEON,"1. annotated dataset by authors;

2. manually defined NLP heuristics as ground truth","TruePositives (TP), False Negatives (FN), False Positives (FP), TrueNegatives (TN), Recall (R), Precision (P), and F-Measure (F1)"
SAFE: A Simple Approach for Feature Extraction from App Descriptions and App Reviews,2017,SAFE,https://github.com/faizalishah/SAFE_REPLICATION (replication package submiited by others),Extracting app features via pre-defined text patterns,"app reviews, app descriptions",Identifying Patterns+ Automated Feature Extraction + Automated Feature Matching,no,,"1. App store mining and analysis: Msr for app stores (not in our SLR list-not related to processing review text)

2. How do users like this feature? a fine grained sentiment analysis of app reviews (no open source link)","1. annotated dataset by authors;

2. manual verification (to evaluate feature matching results)","Recall (R), Precision (P), and F-Measure (F1)"
What would users change in my app? summarizing app reviews for recommending software changes,2016,URM,https://spanichella.github.io/tools.html#surf,Classifying app reviews based on users' intentions  and the review topics.,app reviews,Intention Classification + Topics Classification,no,,evaluate their own tool,survey,correctness rate
What would users change in my app? summarizing app reviews for recommending software changes,2016,SURF,https://spanichella.github.io/tools.html#surf,Summarizing app reviews by visualizing the most informative reviews and recommending software changes,app reviews,Intention Classification + Topics Classification + Sentence Scoring and Extraction + Summary Generation,no,,evaluate their own tool,"survey (to manually validate the classification correctness of data contained in the summaries (Q1 in Table 6) and provide their opinion on (i) the content adequacy (Q9 of Table 6), (ii) the conciseness (Q10 of Table 6), and (iii) the expressiveness (Q12 of Table 6))","Likert scale, percentage"
INFAR: insight extraction from app reviews,2018,INFAR,"Demo Tool Website: https://remine-lab.github.io/paper/infar.html
Demo Video: https://youtu.be/MjcoiyjA5TE (tool website)","Summarizing app reviews with trend analysis and three insights (i.e., salient topics, abnormal topics, and correlated topics)",app reviews,Insight Extraction + Text Template Definition + User Review Prioritization,no,,evaluate their own tool,survey (to evaluate the usefulness of the insight summary generated by INFAR),percentage
Caspar: Extracting and Synthesizing User Stories of Problems from App Reviews,2020,Caspar,https://hguo5.github.io/Caspar/,Summarizing user stories of problems from app reviews,app reviews,"Extracting Events + Synthesizing Event Pairs (Manual labeling, Event encoding, Classification, Synthesis, Manual verification) + Inferring Events (USE+SVM, Bi-LSTM network, Negative sampling, Clustering)",no,,"evaluate their own tool:

Classification: TF-IDF + SVM and USE + SVM;","1. annotated dataset by authors;

2. manual verification (manual verification of Caspar's extraction results)","Accuracy, Precision, Recall"
Experience Report: Understanding Cross-Platform App Issues from User Reviews,2016,CrossMiner,https://github.com/armor-ai/CrossMiner,Summarizing cross-platform app issues from app reviews,app reviews,Clean Review Extraction + Keywords Generation (Training Model + Extracting Keywords + Ranking Reviews) + Visualization,no,,evaluate their own tool,"1. show case studies;
2. use official user forums as ground truth: capture the issue rankings from the official iOS community and Window Phone community",Normalized Discounted Cumulative Gain (NDCG)
User reviews matter! Tracking crowdsourced reviews to support evolution of successful apps,2015,CRISTAL,https://www.cs.wm.edu/semeru/data/ICSME15-cristal/,Tracing app reviews onto app changes for better evolution,"app reviews, commit notes and issue reports",Extracting Issues and Commits + Detecting Links (Linking Informative Reviews and Issues + Linking Informative Reviews and Commits + Linking Issues and Commits + Filtering Links) + Monitoring Crowdsourced Reviews with CRISTAL,"AR-Miner, ReLink",,evaluate their own tool,annotated dataset by authors,"Precision, Recall, and F-measure"
Online App Review Analysis for Identifying Emerging Issues,2018,IDEA,https://github.com/ReMine-Lab/IDEA,Summarizing app reviews for detecting emerging app issues,app reviews,"Phrase Extraction + Emerging Topic Detection (AOLDA, Emerging Topic Detection) + Topic Interpretation (Candidate Extraction, Topic Labeling) + Visualization",no,,"evaluate their own tool:

OLDA, IDEA-R, IDEA-S, IDEA+","1. use app changelogs as ground truth;

2. manual verification (whether the identified app issues of one version can be reflected in the changelogs of the next version)","Precision, Recall, and F-measure"
Listening to the Crowd for the Release Planning of Mobile Apps,2019,CLAP,https://dibt.unimol.it/report/others/clap/,Summarizing app reviews for app release planning,app reviews,Categorizing User Reviews + Clustering Related Reviews + Prioritizing Review Clusters,no,AR-Miner,,"1. annotated dataset by authors;

2. comparing the review clusters generated by CLAP with respect to manually-produced clusters by three industrial developers;

3. assessing the ability of CLAP to recommend changes to implement in sight of the next app releases;

4. semi-structuredinterviews (provided our tool to managers of three Italian software companies to get quantitative and qualitative feedback about the applicability of CLAP in their everyday decision making process)","1. measure the similarity between the two partitions of reviews by CLAP) by using the MoJo eFfectiveness Measure(MoJoFM) [25], a normalized variant of the MoJo distance;

2. Precision, Recall, and F-measure, AUROC"
Identifying Key Features from App User Reviews,2021,KEFE,https://github.com/GIST-NJU/KEFE,Identifying app  features highly correlated to app ratings from app reviews,"app reviews, app descriptions",Feature Extraction + User Review Matching + Regression Analysis,no,,random approach,"1. annotated dataset by authors；

2. use release notes as ground truth.","Precision, Recall, F-measure, Accuracy, Hit Ratio"
Emerging App Issue Identification via Online Joint Sentiment-Topic Tracing,2022,MERIT,https://github.com/armor-ai/MERIT,Summarizing app reviews for detecting emerging app issues,app reviews,Data Preparation (Preprocessing and Filtering + Polarity Word Preparation) + AOBST (Biterm Sentiment-Topic Model + Adaptive Online Joint Sentiment-Topic Tracing) + Emerging Issue Detection (Emerging Topic Identification + Automatic Topic Interpretation +Interpreting Topics with Phrases/ Sentences) + Interpreting Topics with Phrases.,AOBST,IDEA,OLDA,use app changelogs as ground truth,"Precision, Recall, and F1-score"
How can i improve my app? Classifying user reviews for software maintenance and evolution,2015,ARdoc,https://spanichella.github.io/tools.html#ardoc,Classifying app reviews for software maintenance and evolution,app reviews,Taxonomy for Software Maintenance and Evolution + Text Analysis (Preprocessing + Textual Feature Weighting) + Natural Language Processing + Sentiment Analysis + Learning Classifiers,use AR-miner to filter out non-informative reviews when annotating dataset,,"evaluate their own tool:

different combination of NLP, TA and SA approaches",annotated dataset by authors,"Precision, Recall, F-measure"
Analyzing reviews and code of mobile apps for better release planning,2017,URR,https://zenodo.org/records/161652,Classifying app reviews and localizing source code to be changed,"app reviews, source code",Classifying reviews + Linking reviews with source code,no,,evaluate their own tool,"1. annotated dataset by authors；

2. manual verification for the performance of linking reviews & code: we ran the URR prototype using the dataset obtained in the previous step and saved for each review the list of source code artifacts returned as output. We then asked the evaluator to report the set of false positive and false negatives for each review;

3. survey: investigate the perceived difficulty in analysing user reviews and the potential usefulness of our approach","Precision, Recall, and F1-score"
Recommending and Localizing Change Requests for Mobile Apps Based on User Reviews,2017,CHANGEADVISOR,https://sites.google.com/site/changeadvisormobile/,Localizing source code to be changed based on app reviews,"app reviews, source code",User Feedback Classification + Input Preprocessing + User Feedback Clustering + Recommending Source Code Changes,employs ARdoc to extract and classify the informative app review sentences,,a baseline implementation of the tool proposed by Saha et al. [24] in the context of bug localization.,"1. manual verification: to evaluate the cohesiveness of the clusters generated by CHANGE ADVISOR with a Likert scale;

2. survey: (qualitatively evaluated C HANGE A DVISOR by surveying the 10 original developers of the apps in our dataset, who confirmed the actual usefulness of the approach in practice);","Precision, Recall"
Where2Change: Change Request Localization for App Reviews,2021,Where2Change,https://github.com/Jiachi-Chen/ReviewBugLocalization,Localizing source code to be changed based on app reviews and issue reports,"app reviews, source code, issue reports",Selecting Change Request-Related User Feedback + Clustering Change-Related User Feedback + Building a Link between Feedback Clusters and Issue Reports + Change Request Localization Using Enriched Cluster of User Feedback,use SURF to automatically classify the user feedback,CHANGEADVISOR,two IR-based fault localization technologies-BLUiR and BLIA,annotated dataset by exterior developers,"clustering: Homogeneity, Completeness, and V score [29] (Homogeneity, Completeness, and V score: Homogeneity is the ratio of user feedback in a single cluster belonging to the same cluster of gold standard. Completeness measures the ratio of user feedback in a same cluster of gold standard which are assigned to the same cluster produced by algorithms. The lower bound is 0 and the upper bound is 1. V score is the harmonic mean between Homogeneity and Completeness);

linking issue report and app reviews:  Top-N, Precision, Recall, MRR, and MAP"
CHAMP: Characterizing Undesired App Behaviors from User Comments Based on Market Policies,2021,CHAMP,https://github.com/UBCFinder/UBCFinder,Indentifying app reviews revealing   undesired behaviors against app market policies via semantic rules,"app reviews, app market policies",Training Dataset Labelling (Topic Modeling and Topic Labelling + Comment Classification + Manual Inspection) + Automated Semantic Rule Extraction ( Representative Keywords Extraction + Semantic Rule Generation) + Behavior Identification,no,,Similarity-Based Tool,"1. annotated dataset by authors

2. manual verification: For each of the 25 identified perceived behaviors, we randomly select three apps(75 apps in total) and manually verify if indeed the appsviolated the policies as described (steps: installation, app testing tool, static analysis).","Precision, Recall, and F1-score"
Automating App Review Response Generation,2019,RRGen,https://github.com/armor-ai/RRGen,Automating app review response generation,"app reviews, developer responses",RNN Encoder-Decoder Model + Attention Mechanism,no,,"a random selection approach, the basic attentional RNN encoder-decoder (NMT) model [28: Neural machine translation by jointly learning to align and translate];
NNGen (the state-of-the-art approach in commit message generation based on code changes [23: Neuralmachine-translation-based commit message generation: how far are we?])","1. dataset without annotation;
2. survey (evaluate ""grammatical fluency"", ""relevance"", and ""accuracy"" with Likert scale)","BLEU-4 score, Precision"
Automating App Review Response Generation Based on Contextual Knowledge,2021,CoRe,https://bit.ly/3kv6WEl,Automating app review response generation,"app reviews, app descriptions, developer responses",Attentional Encoder-decoder Model + Pointer-generator Model,no,RRGen,"Random Selection, NNGen, PRSummarizer","1. dataset without annotation;
2. survey: Each participant is invited to read 25 user reviews and judge the quality of the responses generated by CoRe, RRGen, and the official app developers (evaluate ""grammatical fluency"", ""relevance"", and ""accuracy"" with Likert scale).","BLEU-4 score, Precision, Rouge, METEOR"
Towards De-Anonymization of Google Play Search Rank Fraud,2021,Dolos,https://github.com/FraudHunt,Attributing fraudulent behaviors to crowdsourcing site profiles by using app reviews,"app reviews, crowdsourcing websites",Fraud Worker Profile Collection (FPC) + Fraud Component Detection (FCD) Module + Component Attribution (CA) Module,no,,evaluate their own tool,"ground truth: reviews posted from the 2,664 attributed, fraud worker controlled user accounts","Precision, Recall, and ""prevalence"": the ratio of the number of fraud labeled accounts to the total number of the app's accounts"
Learning Features that Predict Developer Responses for iOS App Store Reviews,2020,Srisopha et al.,https://github.com/Kamonphop/ESEM20-Replication,Predicting developer responses for app reviews,"app reviews, developer responses",Feature Selection (Rating + Length + Vote + Time + Style + Sentiment + Intention),no,,"Random, MajorityClass",dataset without annotation,AUC scores
Listening to Users' Voice: Automatic Summarization of Helpful App Reviews,2022,SOLAR,https://github.com/monsterLee599/SOLAR,Summarizing app reviews by prioritizing the most informative reviews,app reviews,Review Helpfulness Prediction + Biterm-Based Sentiment-Topic Modeling + Multifactor Topic and Review Ranking,no,"AR-Miner, IDEA",,"1. use app changelogs as ground truth;

2. manual verification (invite three industrial developers to evaluate the consistency between the prioritized reviews and changelogs and also the informativeness).","Precision, Recall, and F1-score;
We also involve the metric infor-score for measuring the informativeness of the prioritized reviews"
Proactive Prioritization of App Issues via Contrastive Learning,2022,PPrior,https://github.com/MultifacetedNLP/PPrior,Prioritizing app issues from app reviews,app reviews,Self-Supervised Training + Contrastive Training + Radius Neighbor Classification,no,,"compare against several state-of-the-art transformer architectures [4], [10], [37], [50] that leverage several strategies to overcome the class imbalance issue [26], [51].  also compare against state-of-the-art approaches for predicting the social media post's popularity [9], [41].","1. using the number of votes each review received in a month after being posted as ground truth: e.g., for binary classification: whether a given user review will receive more than 100 votes in a month or not;

2. assigning each review to three different students and asked them to classify the review whether it highlights any critical issue or not.","Accuracy, F1 score, and MCC score"
Where is Your App Frustrating Users?,2022,SIRA,https://github.com/MeloFancy/SIRA,Summarizing app reviews for presenting problematic app features,app reviews,Problematic Feature Extraction + Problematic Feature Clustering + Visualization,no,"KEFE, Caspar, and SAFE","1. Baselines for Problematic Feature Extraction: BiLSTM-CRF [25 ], a commonly-used technique in NER tasks;
2. Baselines for problematic feature Clustering: K-Means, LDA.","1. annotated dataset by authors;
2. survey: to assess the usefulness of SIRA","1. Metrics for Problematic Feature Extraction: Precision, Recall, and F-measure;
2. Metrics for Problematic Feature Clustering: 
Adjusted Rand Index (ARI): It takes values in [−1, 1], reflecting the degree of overlap between the two clusters;
Normalized Mutual information (NMI): It measures the similarity degree of the two sets of clustering results between 0 (no mutual information) and 1 (perfect correlation)."
Supporting Developers in Addressing Human-centric Issues in Mobile Apps,2022,Khalajzadeh et al.,https://zenodo.org/records/6982529,Classifying human-centric issues in app reviews and issue comments,"app reviews, issue comments in Github",Categories of End-User Human-Centric Issues (Pilot Analysis + Main Analysis) + Automated Classification of End-User Human-Centric Issues (Machine Learning / Deep Learning),no,,evaluate different ML and DL models,"1. annotated dataset by authors;
2. survey: to investigate the usefulness of the automated classification of human-centric issues from the perspective of software/app developers. (量表： Participants' level of agreement)","Accuracy, Precision, Recall, and F1-score"
On the Violation of Honesty in Mobile Apps: Automated Detection and Categories,2022,Obie et al.,https://anonymous.4open.science/r/ml_app_reviews-3ED6,Identifying app dishonest behaviors from app reviews,app reviews,Automatic Classification of Honesty Violations + Categories of Honesty Violations,no,,compared five machine learning models,annotated dataset by authors,"Accuracy, Precision, Recall, and F1-score"
The Use of Pretrained Model for Matching App Reviews and Bug Reports,2022,Wang et al.,https://github.com/Cesare3/Matching-App-Reviews-and-Bug-Report,Matching app reviews and bug reports,"app reviews, bug reports",Noun and Verb Selection + Pretrained model,no,DeepMatcher,"compare four major pretrained models (including T5, Sentence T5, Sentence MiniLM, Sentence BERT and so on)",annotated dataset by authors,"Precision, Hit ratio"
Hark: A Deep Learning System for Navigating Privacy Feedback at Scale,2022,Hark,https://github.com/google/hark,Summarizing privacy issues from app reviews,app reviews,Privacy Feedback Classifier + Issue Generation (for individual reviews) + Theme Creation (for a body of reviews; Issue Grouping + Theme Title Creation) + Improving Navigability (Emotions Model + Feedback Quality Model),no,,evaluate different ML and DL models,"1. annotated dataset by authors;
2. human evaluation to assess the quality of the generated titles.","Accuracy, Coverage"
Unsupervised Summarization of Privacy Concerns in Mobile Application Reviews,2022,Ebrahimi et al.,https://seel.cse.lsu.edu/data/ase22.zip,Summarizing privacy issues from app reviews,app reviews,Privacy Term Extraction + Privacy Review Summarization,no,,"evaluate their own tool:

Hybrid.TF.IDF, Hybrid.TF.IDF+GloVe",dataset without annotation,Hybrid.TF.IDF score
Hierarchical Bayesian multi-kernel learning for integrated classification and summarization of app reviews,2022,HMK-RVM,https://tinyurl.com/qup3h4l,Summarizing app reviews by leveraging the hierarchical relationships between app review labels,app reviews,Hierarchical User Review Classiication + Multi-Kernel Relevance Vector Machines,no,"AR-Miner, 
ARdoc","evaluate their own tool:
1. hierarchical classiication versus lat classiication;
2. classification approaches: RVM, MK-RVM, Maalej [25: Bug report, feature request, or simply praise? On automatically classifying app reviews, 这篇文章被删了：maalej2016automatic是 maalej2015bug 的扩展文章，把maalej2015bug作为重复工作删掉, maalej2016automatic 链接失效了，所以不在36 tools 里面].","previous researchers' datasets: Maalej dataset [24, 25], Panichella dataset [34, 40]. The second dataset did not procide ground truth and is annotated by authors.","AUC scores, Precision, Recall, and F1-score"
RE-SWOT: From User Feedback to Requirements via Competitor Analysis,2019,RE-SWOT,https://github.com/RELabUU/RE-SWOT,Generating and visualizing user requirements from app reviews with SWOT model,app reviews,RE-SWOT Method (Identify Features and Transform Ratings + Calculate FPS per Feature + Generate RE-SWOT Matrix + Generate Requirements + Visualization Module),no,,evaluate their own tool,survey: semi-structured interview (没有量表) to determine how practitioners find RE-SWOT supportive to requirements elicitation through competitor analysis.,/
Mining non-functional requirements from App store reviews,2019,MARC 3.0,https://github.com/seelprojects/MARC-3.0,Identifying non-functional requirements from app reviews,app reviews,Manual Classification + Automated Classification + Dictionary Based Classification,no,,"evaluate their own tool:

1. Naive Bayes (NB) and Support Vector Machines (SVM);
2. Text pre-processing, App category, Sentiment analysis.",annotated dataset by authors,"Precision, Recall, F-measure.
Subset Accuracy (SA): also referred to as Exact Match (EM), is the number of predictions that are completely correct divided by the total number of classified data instances.
Hamming Score (HS): is the proportion of correctly predicted labels over the total number (predicted and actual) of labels identified for a data instance.
Hamming Loss (HL): is a measure of how many times on average, a class label is incorrectly predicted. This measure takes into account the prediction error (an incorrectly predicted label) along with the missing error (a label not predicted), normalized over the total number of classes and the total number of examples (Sorower 2010). HL = 0 implies that there is no error in the prediction. Practically, the smaller the value of Hamming Loss, the better an algorithm performs."
"Prioritizing user concerns in app reviews—A study of requests for new features, enhancements and bug fixes",2022,Malgaonkar et al.,https://tinyurl.com/y4hynj5j,Prioritizing app reviews,app reviews,Regression-based prioritization technique (Entropy + Frequency + TF-IDF + Sentiment analysis),use AR-miner to filter informative reviews,,evaluate their own tool,"manual verification: authors (internal evaluation), app users with with software engineering and development background (external evaluation)",accuracy and time
Featcompare: Feature comparison for competing mobile apps leveraging user reviews,2021,FeatCompare,https://github.com/maramassi/suppmaterial-featcompare,Comparing app features with competitive apps via app reviews,app reviews,Data Preprocessor + Global-local Sensitive Feature Extractor (GLFE) + Rating Aggregator,use AR-miner to remove non-informative reviews,,"evaluate their own tool:
ABAE, GLFE (extend ABAE by proposing a threshold mechanism)","1. annotated dataset by authors;
2. a survey with 107 mobile developers to investigate how the mobile developers perform competitor analysis in practice and verify the applicability of our approach.","Precision, Recall, and F1-score"
Using frame semantics for classifying and summarizing application store reviews,2018,MARC 2.0,https://github.com/seelprojects/MARC-2.0,Summarizing app reviews by leveraging frame semantics,app reviews,App Review Classification + Review Summarization,no,,"evaluate their own tool:

1. BOW/BOF + NB/SVM;",annotated dataset by authors,"Precision, Recall, and F1-score"
